import ioimport osfrom enum import IntEnumfrom typing import List, Callable, Anyfrom dizer3.utils import parenthetical_markers, attribution_verbsdef segment_sentence(annotated_sentence):    breaks = []    skipped_indexes = []    # search for candidates points to segment    for i, token in enumerate(annotated_sentence):        if i in skipped_indexes:            continue        validate = lambda rules, start_token=i, direction=Direction.Forward: \            _validate_tokens(rules, start_token, annotated_sentence, direction)        # relative clauses if with comma?        # and annotated_sentence[i-1]['pos'] == "PUNCT":        if "PronType=Rel" in token['morpho']:            breaks.append(i)        # gerund or participle clauses        # and "PUNCT" in token['pos']:        # if ("VerbForm=Ger" in token['morpho'] or "VerbForm=Part" in token['morpho']) and i > 0 and annotated_sentence[i-1]['pos'] not in ['VERB', 'AUX']:        #     breaks.append(i)        # gerund clauses, except if preceded by "estar" verb        preceded_by_estar_verb_second = validate([  # TODO Rename this rule to something more semantic            lambda idx, t: 'estar' == t['lemma']        ], start_token=i - 2)        preceded_by_estar_or_auxiliar_verb = validate([            lambda idx, t: 'estar' == t['lemma'] or 'AUX' in t['pos']        ], start_token=i - 1)        if "VerbForm=Ger" in token['morpho'] and not preceded_by_estar_verb_second and \                not preceded_by_estar_or_auxiliar_verb and 'AUX' not in token['pos']:            # validate if gerund is followed by a adverb            if validate([lambda idx, t: 'ADV' in t['pos']], start_token=i - 1):                breaks.append(i - 1)            else:                breaks.append(i)        # coordinate conjunction        if "CCONJ" in token['pos']:            breaks.append(i)        # subordinate conjunction        if "SCONJ" in token['pos']:            breaks.append(i)        # infinite clauses        if "para" == token['lemma'] and len(annotated_sentence) > i and "VerbForm=Inf" in annotated_sentence[i + 1][            'morpho']:            breaks.append(i)        # attributions segments        if validate([            lambda idx, t: 'PUNCT' in t['pos'],            lambda idx, t: "segundo" == t['lemma'] or "conforme" == t['lemma']        ], direction=Direction.Backward):            breaks.append(i)        # através de        if validate([lambda idx, t: 'através' == t['lemma'], lambda idx, t: 'de' == t['lemma']]):            breaks.append(i)        # adverbial after comma        if validate([            lambda idx, t: ',' in t['token'],            lambda idx, t: 'ADV' == t['pos']        ], direction=Direction.Backward):            breaks.append(i)        # visto que        if validate([            lambda idx, t: t['token'].lower() == 'visto',            lambda idx, t: t['token'].lower() == 'que'        ]):            breaks.append(i)        # parenthetical        if token['token'] in ['(', '[', '{']:            j = len(annotated_sentence)            while j > i:                j -= 1                if annotated_sentence[j]['token'] in [')', ']', '}'] and j not in breaks:                    if (j + 1) < len(annotated_sentence) - 1 and \                            ("PUNCT" in annotated_sentence[j + 1]['pos'] or                             "CCONJ" in annotated_sentence[j + 1]['pos']):                        breaks.append(j + 2)                    else:                        breaks.append(j + 1)                    break        if token['token'] in [',', ';', ':']:            breaks.append(i + 1)        # (de forma a / de maneira a) + VINF        if validate([            lambda idx, t: t['lemma'] == 'de',            lambda idx, t: t['lemma'] == 'forma' or t['lemma'] == 'maneira',            lambda idx, t: t['lemma'] == 'a',            lambda idx, t: "VerbForm=Inf" in t['morpho']        ]):            breaks.append(i)            skipped_indexes.extend(range(i+1, i+4))        # durante        if validate([            lambda idx, t: t['token'].lower() == 'durante'        ]):            breaks.append(i)        # conjunctions        for conjunction in ['de forma que', 'de maneira que', 'à medida que', 'visto que', 'para que', 'por conta de', 'tão logo']:            rules = []            for word in conjunction.split():                rules.append(lambda idx, t, word=word: t['token'].lower() == word.lower())            if validate(rules):                breaks.append(i)                skipped_indexes.extend(range(i+1, i+len(conjunction)))        for conjunction in ['caso', 'pois', 'quando', 'após']:            rules = [lambda idx, t, conjunction=conjunction: t['token'].lower() == conjunction.lower()]            if validate(rules):                breaks.append(i)        # a partir de (and variations)        if validate([            lambda idx, t: t['token'].lower() == 'a',            lambda idx, t: t['token'].lower() == 'partir',            lambda idx, t: t['lemma'].lower() == 'de',        ]):            breaks.append(i)            skipped_indexes.extend(range(i + 1, i + 3))    segments = []    segment = []    for i, token in enumerate(annotated_sentence):        if i in breaks:            segments.append(segment)            segment = []        segment.append(token)    segments.append(segment)    # segments = join_by_verbs(segments)    segments = parentheticals(segments)    # segments = attribution(segments)    # remove empties    empties = []    for i, segment in enumerate(segments):        if len(segment) == 0:            empties.append(i)    segments = [s for i, s in enumerate(segments) if i not in empties]    return segmentsdef attribution(segments):    new_segments = []    for segment in segments:        new_segments.append([])        i = 0        while i < len(segment):            token = segment[i]            if i == (len(segment) - 1):                new_segments[-1].extend([token])                i += 1            # PROPN ... attrib_verb que            elif token['pos'] in ['PROPN', 'NOUN']:                start = i                i += 1                while i < (len(segment) - 1) and segment[i]['pos'] in ['PROPN', 'NOUN']:                    i += 1                if i < (len(segment) - 1) and segment[i]['lemma'] in attribution_verbs:                    if len(new_segments[-1]) > 0:                        new_segments.append([])                    i += 1                for j in range(start, i):                    new_segments[-1].extend([segment[j]])            # PUNCT attrib_verb            elif token['pos'] == 'PUNCT' and segment[i + 1]['lemma'] in attribution_verbs:                # segment after i                new_segments[-1].extend([token])                new_segments.append([])                i += 1            # TODO: de acordo com .... at start and end of a sentence...            else:                new_segments[-1].extend([token])                i += 1    return new_segmentsdef search_correspondence(segment, i, token):    parentheticals    end_token = parenthetical_markers.get(token)    for j, token in enumerate(segment[i:]):        if token['token'] == end_token:            if segment[j + i]['pos'] == 'PUNCT':                return j + i + 1            return j + i    return Nonedef parentheticals(segments):    new_segments = []    for segment in segments:        start = None        new_segments.append([])        i = 0        while i < len(segment):            token = segment[i]            if token['token'] in parenthetical_markers.keys():                start = i                end = search_correspondence(segment, i, token['token'])                if end is not None:                    new_segments.append([])                    for j in range(start, end):                        try:                            new_segments[-1].extend([segment[j]])                        except:                            pass  # last token                    new_segments.append([])                    i = end                else:                    new_segments[-1].extend([token])                    i += 1            else:                new_segments[-1].extend([token])                i += 1    return new_segmentsdef find_direction(mapping, curr):    for i, v in enumerate(reversed(mapping[:curr + 1])):        if v:            return 'backward', len(mapping) - i    for i, v in enumerate(mapping[curr:]):        if v:            return 'forward', i    return 'not_found', -1def join_by_verbs(segments):    # verify if each segment has a verb    map_verbs = []    for segment in segments:        has_verb = False        for token in segment:            if token['pos'] in ['VERB', 'AUX']:  # AUX too?                has_verb = True        map_verbs.append(has_verb)    new_segments = []    i = 0    while i < len(segments):        if map_verbs[i] == False:            d, index = find_direction(map_verbs, i)            if d == 'backward':                new_segments[-1].extend(segments[i])                i += 1            elif d == 'forward':                new_segments.append(segments[i])                for s in segments[i + 1:index + 1]:                    new_segments[-1].extend(s)                i = index + 1            elif d == 'not_found':                new_segments.append(segments[i])                for s in segments[i + 1:]:                    new_segments[-1].extend(s)                i = len(segments)        else:            new_segments.append(segments[i])            i += 1    return new_segments# def _get_conjunctions():#     with io.open(os.path.join(current_path, "resources/listConjunctions.txt"), encoding='utf8') as file:#         for line in file.readlines():#             conjunction, conjunction_group, conjunction_type = line.split(',')#             if conjunction_group != 'Coordenativa' and \#                     not (conjunction_type == 'Aditiva' or conjunction_type == 'Alternativa'):#                 yield conjunction### current_path = os.path.dirname(os.path.realpath(__file__))## conjunctions = list(_get_conjunctions())class Direction(IntEnum):    Forward = 1    Backward = 2def _validate_tokens(rules: List[Callable[[int, Any], bool]], start_token: int, sentence: List,                     direction: Direction = Direction.Forward):    end_token = start_token + len(rules) if direction == Direction.Forward else start_token - len(rules) + 1    indexes = range(start_token, end_token) if direction == Direction.Forward else range(end_token, start_token + 1)    if start_token > len(sentence) or end_token < 0 or end_token > len(sentence):        return False    for index, rule in zip(indexes, rules):        if not rule(index, sentence[index]):            return False    return True